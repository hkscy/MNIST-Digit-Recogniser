{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chris Hicks 2020\n",
    "# https://www.kaggle.com/c/digit-recognizer\n",
    "#\n",
    "# The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "# Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. \n",
    "# Each pixel has a single pixel-value associated with it, an integer between 0 and 255, inclusive.\n",
    "# The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was \n",
    "# drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "#\n",
    "# test.csv is the same as the training set, except that it does not contain the \"label\" column.\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "#from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset, returning numpy arrays comprising the respective train data, \n",
    "# train labels and test data.\n",
    "# Inputs: filename of training data in CSV format\n",
    "#         tuple (train, cv, test) enciding data split as whole digit percentages e.g. 80-10-10 \n",
    "# Output: numpy arrays comprising the respective train data, train labels, test data and test labels\n",
    "def load_dataset(train_filename, split=(90,10)):\n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(train_filename)\n",
    "    labels = train_data['label']\n",
    "    labels = labels.to_numpy()\n",
    "    train_data = train_data.drop('label', axis=1)\n",
    "    train_data = train_data.to_numpy()\n",
    "    \n",
    "    # Split boundaries\n",
    "    n_train = math.floor(train_data.shape[0]*(split[0]/100))\n",
    "    \n",
    "    train_X = train_data[:n_train]\n",
    "    train_labels = labels[:n_train]\n",
    "    \n",
    "    cv_X = train_data[n_train:]\n",
    "    cv_labels = labels[n_train:]\n",
    "    \n",
    "    return train_X, train_labels, cv_X, cv_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creates a list of random minibatches from (X, Y), each of size mini_batch_size\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column vector Y comprising n_classes possible values into a one-hot encoded array.\n",
    "# Returns: Yxn_classes dimensional one-hot encoded array\n",
    "def convert_to_one_hot(Y, n_classes):\n",
    "    return np.eye(n_classes)[Y.reshape(-1)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF placeholders\n",
    "# Returns:\n",
    "#    X : placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
    "#    Y : placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, (n_x, None), name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, (n_y, None), name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise parameters for neural network graph\n",
    "# Inputs:\n",
    "#     image_size : total number of flattened pixel values in image\n",
    "#     n_classes : number of output classes in network\n",
    "# Returns:\n",
    "#    parameters W1,...WL, b1,...,bL\n",
    "def initialise_parameters(image_size, n_classes):\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [25,image_size], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [n_classes,12], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", [n_classes,1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "# Inputs:\n",
    "#              X : input dataset placeholder, of shape (input size, number of examples)\n",
    "#     parameters : python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "#                  the shapes are given in initialize_parameters\n",
    "#\n",
    "# Returns:\n",
    "#     Z3 : the output of the last LINEAR unit.\n",
    "def forward_propagation(X, parameters):\n",
    "\n",
    "    # Retrieve the parameters from the parameters dictionary\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      \n",
    "    A1 = tf.nn.relu(Z1)                                   \n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     \n",
    "    A2 = tf.nn.relu(Z2)                                   \n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3) \n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the cost function\n",
    "# Inputs:\n",
    "#    Z3 : Output from forward propagation\n",
    "#    Y : Ground truth labels\n",
    "# Output:\n",
    "#    cost : Tensor of the cost function\n",
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "def model(train_X, train_labels, cv_X, cv_labels,\n",
    "          learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    costs = [] # To keep track of costs\n",
    "    \n",
    "    # Meta data\n",
    "    (n_x, m) = train_X.shape # n_x = input size, m = number of examples\n",
    "    n_y = train_labels.shape[0]\n",
    "    \n",
    "    # Build graph\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = initialise_parameters(n_x, n_y)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Define optimiser\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialise all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialisation\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.                           # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(train_X, train_labels, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # Run the graph on a minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_X, Y: train_labels}))\n",
    "        print (\"CV Accuracy:\", accuracy.eval({X: cv_X, Y: cv_labels}))\n",
    "        \n",
    "        \n",
    "        return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    train_filename = \"train.csv\"\n",
    "    test_filename = \"test.csv\"\n",
    "    train_X, train_labels, cv_X, cv_labels = load_dataset(train_filename)\n",
    "    \n",
    "    print(\"There are {}, {} train and CV examples respectively.\".format(train_X.shape[0], cv_X.shape[0]))\n",
    "    \n",
    "    n_classes = 10 \n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 200\n",
    "    minibatch_size = 32\n",
    "    print_cost = True\n",
    "    \n",
    "    # E.g. view a digit from the training set and print the corresponding label\n",
    "    # index = 4\n",
    "    # plt.imshow(train_X.reshape(train_X.shape[0],28,28)[index])\n",
    "    # print(\"Label is {}\".format(train_labels[index]))\n",
    "    \n",
    "    # Flatten the training, cv and test images\n",
    "    train_X_flatten = train_X.reshape(train_X.shape[0], -1).T\n",
    "    cv_X_flatten = cv_X.reshape(cv_X.shape[0], -1).T\n",
    "    \n",
    "    # Normalize image vectors\n",
    "    train_X = train_X_flatten/255.\n",
    "    cv_X = cv_X_flatten/255.\n",
    "    \n",
    "    # Convert labels to one hot matrices\n",
    "    train_labels = convert_to_one_hot(train_labels, n_classes)\n",
    "    cv_labels = convert_to_one_hot(cv_labels, n_classes)\n",
    "    \n",
    "    # Train model on train data and then test it on the `cv' data\n",
    "    parameters, accuracy = model(train_X, train_labels, cv_X, cv_labels,\n",
    "                                   learning_rate = learning_rate, num_epochs = num_epochs, \n",
    "                                   minibatch_size = minibatch_size, print_cost = print_cost)\n",
    "    \n",
    "    # Load test data, run model and write file needed for Kaggle submission\n",
    "    \n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37800, 4200 train and CV examples respectively.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-0fee97dc504a>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 0.500099\n"
     ]
    }
   ],
   "source": [
    "parameters = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
